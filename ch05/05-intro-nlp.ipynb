{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a47a0ed",
   "metadata": {},
   "source": [
    "# 5장. 자연어 처리 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00adecd8",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/rickiepark/aiml4coders/blob/main/ch05/05-intro-nlp.ipynb\"><img src=\"https://jupyter.org/assets/share.png\" width=\"61\" />주피터 노트북 뷰어로 보기</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/aiml4coders/blob/main/ch05/05-intro-nlp.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩(Colab)에서 실행하기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3eabd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노트북이 코랩에서 실행 중인지 체크합니다.\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget -q https://github.com/rickiepark/aiml4coders/raw/main/ch05/binary-emotion.csv\n",
    "    !wget -q https://github.com/rickiepark/aiml4coders/raw/main/ch05/sarcasm.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db450aa",
   "metadata": {},
   "source": [
    "## 언어를 숫자로 인코딩하기\n",
    "\n",
    "### 토큰화 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfd3116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'rainy': 6}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'Today is a sunny day',\n",
    "    'Today is a rainy day'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af926e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'today', 2: 'is', 3: 'a', 4: 'day', 5: 'sunny', 6: 'rainy'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825ff630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('today', 2),\n",
       "             ('is', 2),\n",
       "             ('a', 2),\n",
       "             ('sunny', 1),\n",
       "             ('day', 2),\n",
       "             ('rainy', 1)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982e5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'Today is a sunny day',\n",
    "    'Today is a rainy day',\n",
    "    'Is it sunny today?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d79864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'rainy': 6, 'it': 7}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809388d",
   "metadata": {},
   "source": [
    "### 문장을 시퀀스로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d0c4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 5, 4], [1, 2, 3, 6, 4], [2, 7, 5, 1]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51a67f",
   "metadata": {},
   "source": [
    "#### OOV 토큰 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "999b4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'Today is a snowy day',\n",
    "    'Will it be rainy tomorrow?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6650ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'rainy': 6, 'it': 7}\n",
      "[[1, 2, 3, 4], [7, 6]]\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "print(word_index)\n",
    "print(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0069f9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'today': 2, 'is': 3, 'a': 4, 'sunny': 5, 'day': 6, 'rainy': 7, 'it': 8}\n",
      "[[2, 3, 4, 1, 6], [1, 8, 1, 7, 1]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "print(word_index)\n",
    "print(test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fed6ae-3a88-4994-b674-7c5e38b69c2e",
   "metadata": {},
   "source": [
    "##### `TextVectorization` 층 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cfd43e-0f95-4744-9ea9-98080645a349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 04:29:07.062086: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tv = keras.layers.TextVectorization(max_tokens=100)\n",
    "tv.adapt(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b401a40e-5c12-4952-982a-b0f41726dccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'today', 'is', 'sunny', 'day', 'a', 'rainy', 'it']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e71a16-9361-4fff-a6f8-2892e506920a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 6, 1, 5],\n",
       "       [1, 8, 1, 7, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tv(test_data)\n",
    "test_seq.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d2213",
   "metadata": {},
   "source": [
    "#### 패딩 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0471db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'Today is a sunny day',\n",
    "    'Today is a rainy day',\n",
    "    'Is it sunny today?',\n",
    "    'I really enjoyed walking in the snow today'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d2330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5, 6], [2, 3, 4, 7, 6], [3, 8, 5, 2], [9, 10, 11, 12, 13, 14, 15, 2]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd9e61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93accd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  2  3  4  5  6]\n",
      " [ 0  0  0  2  3  4  7  6]\n",
      " [ 0  0  0  0  3  8  5  2]\n",
      " [ 9 10 11 12 13 14 15  2]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c83fe1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4  5  6  0  0  0]\n",
      " [ 2  3  4  7  6  0  0  0]\n",
      " [ 3  8  5  2  0  0  0  0]\n",
      " [ 9 10 11 12 13 14 15  2]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02d366c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4  5  6  0]\n",
      " [ 2  3  4  7  6  0]\n",
      " [ 3  8  5  2  0  0]\n",
      " [11 12 13 14 15  2]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen=6)\n",
    "\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6b56656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4  5  6  0]\n",
      " [ 2  3  4  7  6  0]\n",
      " [ 3  8  5  2  0  0]\n",
      " [ 9 10 11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen=6, truncating='post')\n",
    " \n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0e56e",
   "metadata": {},
   "source": [
    "## 실제 데이터 다루기\n",
    "\n",
    "### 텐서플로 데이터셋에서 텍스트 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2228be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "imdb_sentences = []\n",
    "train_data = tfds.as_numpy(tfds.load('imdb_reviews', split=\"train\"))\n",
    "for item in train_data:\n",
    "    imdb_sentences.append(str(item['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e45a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(imdb_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(imdb_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력이 길어 노트북에서 제외함\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
    "             \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\",\n",
    "             \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\",\n",
    "             \"he\", \"hed\", \"hes\", \"her\", \"here\", \"heres\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n",
    "             \"hows\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n",
    "             \"lets\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",\n",
    "             \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"shed\", \"shell\", \"shes\", \"should\",\n",
    "             \"so\", \"some\", \"such\", \"than\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\",\n",
    "             \"there\", \"theres\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"this\", \"those\", \"through\",\n",
    "             \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"were\",\n",
    "             \"what\", \"whats\", \"when\", \"whens\", \"where\", \"wheres\", \"which\", \"while\", \"who\", \"whos\", \"whom\", \"why\",\n",
    "             \"whys\", \"with\", \"would\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\",\n",
    "             \"yourselves\"]\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "imdb_sentences = []\n",
    "train_data = tfds.as_numpy(tfds.load('imdb_reviews', split=\"train\"))\n",
    "for item in train_data:\n",
    "    sentence = str(item['text'].decode('UTF-8').lower())\n",
    "    sentence = sentence.replace(\",\", \" , \")\n",
    "    sentence = sentence.replace(\".\", \" . \")\n",
    "    sentence = sentence.replace(\"-\", \" - \")\n",
    "    sentence = sentence.replace(\"/\", \" / \")\n",
    "    soup = BeautifulSoup(sentence)\n",
    "    sentence = soup.get_text()\n",
    "    words = sentence.split()\n",
    "    filtered_sentence = \"\"\n",
    "    for word in words:\n",
    "        word = word.translate(table)\n",
    "        if word not in stopwords:\n",
    "            filtered_sentence = filtered_sentence + word + \" \"\n",
    "    imdb_sentences.append(filtered_sentence)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=25000)\n",
    "tokenizer.fit_on_texts(imdb_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(imdb_sentences)\n",
    "# 출력이 길어 노트북에서 제외함\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c61cab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[516, 5229, 147], [516, 6489, 147], [5229, 516]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'Today is a sunny day',\n",
    "    'Today is a rainy day',\n",
    "    'Is it sunny today?'\n",
    "]\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "263a4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today sunny day\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in tokenizer.word_index.items()])\n",
    "\n",
    "decoded_review = ' '.join([reverse_word_index.get(i, '?') for i in sequences[0]])\n",
    "\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8284f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today sunny day\n"
     ]
    }
   ],
   "source": [
    "decoded_review = ' '.join([tokenizer.index_word.get(i, '?') for i in sequences[0]])\n",
    "\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c14e15",
   "metadata": {},
   "source": [
    "#### IMDb 보조단어 데이터셋 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96019158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    'imdb_reviews/subwords8k', \n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f66c0832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전 크기: 8185\n"
     ]
    }
   ],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "print ('어휘 사전 크기: {}'.format(encoder.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력이 길어 노트북에서 제외함\n",
    "print(encoder.subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2513f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩된 문자열: [6427, 4869, 9, 4, 2365, 1361, 606]\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Today is a sunny day'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print ('인코딩된 문자열: {}'.format(encoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a3afb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tod\n"
     ]
    }
   ],
   "source": [
    "print(encoder.subwords[6426])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "feaf280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_string = encoder.encode(sample_string)\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "test_string = encoder.decode([6427, 4869, 9, 4, 2365, 1361, 606])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4a398",
   "metadata": {},
   "source": [
    "### CSV 파일에서 텍스트 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8233cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "sentences=[]\n",
    "labels=[]\n",
    "with open('binary-emotion.csv', encoding='UTF-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=\",\")\n",
    "    for row in reader:\n",
    "        labels.append(int(row[0]))\n",
    "        sentence = row[1].lower()\n",
    "        sentence = sentence.replace(\",\", \" , \")\n",
    "        sentence = sentence.replace(\".\", \" . \")\n",
    "        sentence = sentence.replace(\"-\", \" - \")\n",
    "        sentence = sentence.replace(\"/\", \" / \")\n",
    "        soup = BeautifulSoup(sentence)\n",
    "        sentence = soup.get_text()\n",
    "        words = sentence.split()\n",
    "        filtered_sentence = \"\"\n",
    "        for word in words:\n",
    "            word = word.translate(table)\n",
    "            if word not in stopwords:\n",
    "                filtered_sentence = filtered_sentence + word + \" \"\n",
    "        sentences.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ba6bd",
   "metadata": {},
   "source": [
    "#### 훈련 세트와 테스트 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a92f60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 28000\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43b18439",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "max_length = 10\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, \n",
    "                                padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b483af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 3257, 47, 4770, 613, 508, 951, 423]\n",
      "[  18 3257   47 4770  613  508  951  423    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(training_sequences[0])\n",
    "print(training_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a69adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력이 길어 노트북에서 제외함\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c7f86",
   "metadata": {},
   "source": [
    "### JSON 파일에서  텍스트 읽기\n",
    "\n",
    "#### JSON 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aff1eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "    for item in datastore:\n",
    "        sentence = item['headline'].lower()\n",
    "        label= item['is_sarcastic']\n",
    "        link = item['article_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0282c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "for item in datastore:\n",
    "    sentence = item['headline'].lower()\n",
    "    sentence = sentence.replace(\",\", \" , \")\n",
    "    sentence = sentence.replace(\".\", \" . \")\n",
    "    sentence = sentence.replace(\"-\", \" - \")\n",
    "    sentence = sentence.replace(\"/\", \" / \")\n",
    "    soup = BeautifulSoup(sentence)\n",
    "    sentence = soup.get_text()\n",
    "    words = sentence.split()\n",
    "    filtered_sentence = \"\"\n",
    "    for word in words:\n",
    "        word = word.translate(table)\n",
    "        if word not in stopwords:\n",
    "            filtered_sentence = filtered_sentence + word + \" \"\n",
    "    sentences.append(filtered_sentence)\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "778e5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 23000\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "max_length = 10\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, \n",
    "                                padding=padding_type, truncating=trunc_type)\n",
    "# 출력이 길어 노트북에서 제외함\n",
    "print(word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
