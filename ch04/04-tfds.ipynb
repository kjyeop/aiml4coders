{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3105de88",
   "metadata": {},
   "source": [
    "# 4장. 텐서플로 데이터셋으로 공개 데이터 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e8541",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/rickiepark/aiml4coders/blob/main/ch04/04-tfds.ipynb\"><img src=\"https://jupyter.org/assets/share.png\" width=\"61\" />주피터 노트북 뷰어로 보기</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/aiml4coders/blob/main/ch04/04-tfds.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩(Colab)에서 실행하기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5f7ba",
   "metadata": {},
   "source": [
    "## TFDS 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bee79a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 09:01:01.551627: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 29.45 MiB (download: 29.45 MiB, generated: 36.42 MiB, total: 65.87 MiB) to /home/studio-lab-user/tensorflow_datasets/fashion_mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50b4d7f99b144a5b266970444a9edfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c513d7fab340b7afad7da3f7525b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8941fcd8ec704db5bdffad5dc5dab6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 09:01:06.574703: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/studio-lab-user/tensorflow_datasets/fashion_mnist/3.0.1.incompleteEZX2UV/fashion_mnist-train.t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/studio-lab-user/tensorflow_datasets/fashion_mnist/3.0.1.incompleteEZX2UV/fashion_mnist-test.tf…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset fashion_mnist downloaded and prepared to /home/studio-lab-user/tensorflow_datasets/fashion_mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "mnist_data = tfds.load(\"fashion_mnist\")\n",
    "for item in mnist_data:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a439642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "mnist_train = tfds.load(name=\"fashion_mnist\", split=\"train\")\n",
    "assert isinstance(mnist_train, tf.data.Dataset)\n",
    "print(type(mnist_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d04694b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['image', 'label'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 09:01:27.521545: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for item in mnist_train.take(1):\n",
    "    print(type(item))\n",
    "    print(item.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1594fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['image', 'label'])\n",
      "tf.Tensor(\n",
      "[[[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [ 77]\n",
      "  [227]\n",
      "  [227]\n",
      "  [208]\n",
      "  [210]\n",
      "  [225]\n",
      "  [216]\n",
      "  [ 85]\n",
      "  [ 32]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 61]\n",
      "  [100]\n",
      "  [ 97]\n",
      "  [ 80]\n",
      "  [ 57]\n",
      "  [117]\n",
      "  [227]\n",
      "  [238]\n",
      "  [115]\n",
      "  [ 49]\n",
      "  [ 78]\n",
      "  [106]\n",
      "  [108]\n",
      "  [ 71]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 81]\n",
      "  [105]\n",
      "  [ 80]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 64]\n",
      "  [ 44]\n",
      "  [ 21]\n",
      "  [ 13]\n",
      "  [ 44]\n",
      "  [ 69]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 80]\n",
      "  [114]\n",
      "  [ 80]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 26]\n",
      "  [ 92]\n",
      "  [ 69]\n",
      "  [ 68]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 71]\n",
      "  [ 74]\n",
      "  [ 83]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 83]\n",
      "  [ 77]\n",
      "  [108]\n",
      "  [ 34]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 55]\n",
      "  [ 92]\n",
      "  [ 69]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 71]\n",
      "  [ 71]\n",
      "  [ 77]\n",
      "  [ 69]\n",
      "  [ 66]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 80]\n",
      "  [ 78]\n",
      "  [ 94]\n",
      "  [ 63]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 63]\n",
      "  [ 95]\n",
      "  [ 66]\n",
      "  [ 68]\n",
      "  [ 72]\n",
      "  [ 72]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 77]\n",
      "  [106]\n",
      "  [ 61]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 80]\n",
      "  [108]\n",
      "  [ 71]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 71]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 78]\n",
      "  [ 72]\n",
      "  [ 85]\n",
      "  [128]\n",
      "  [ 64]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 88]\n",
      "  [120]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 77]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 83]\n",
      "  [ 83]\n",
      "  [ 66]\n",
      "  [111]\n",
      "  [123]\n",
      "  [ 78]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 85]\n",
      "  [134]\n",
      "  [ 74]\n",
      "  [ 85]\n",
      "  [ 69]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 81]\n",
      "  [ 75]\n",
      "  [ 61]\n",
      "  [151]\n",
      "  [115]\n",
      "  [ 91]\n",
      "  [ 12]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 10]\n",
      "  [ 85]\n",
      "  [153]\n",
      "  [ 83]\n",
      "  [ 80]\n",
      "  [ 68]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 68]\n",
      "  [ 61]\n",
      "  [162]\n",
      "  [122]\n",
      "  [ 78]\n",
      "  [  6]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 30]\n",
      "  [ 75]\n",
      "  [154]\n",
      "  [ 85]\n",
      "  [ 80]\n",
      "  [ 71]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 49]\n",
      "  [191]\n",
      "  [132]\n",
      "  [ 72]\n",
      "  [ 15]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 58]\n",
      "  [ 66]\n",
      "  [174]\n",
      "  [115]\n",
      "  [ 66]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [ 78]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 66]\n",
      "  [ 49]\n",
      "  [222]\n",
      "  [131]\n",
      "  [ 77]\n",
      "  [ 37]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 69]\n",
      "  [ 55]\n",
      "  [179]\n",
      "  [139]\n",
      "  [ 55]\n",
      "  [ 92]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 78]\n",
      "  [ 74]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 80]\n",
      "  [ 64]\n",
      "  [ 55]\n",
      "  [242]\n",
      "  [111]\n",
      "  [ 95]\n",
      "  [ 44]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 74]\n",
      "  [ 57]\n",
      "  [159]\n",
      "  [180]\n",
      "  [ 55]\n",
      "  [ 92]\n",
      "  [ 64]\n",
      "  [ 72]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 55]\n",
      "  [ 66]\n",
      "  [255]\n",
      "  [ 97]\n",
      "  [108]\n",
      "  [ 49]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 74]\n",
      "  [ 66]\n",
      "  [145]\n",
      "  [153]\n",
      "  [ 72]\n",
      "  [ 83]\n",
      "  [ 58]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 80]\n",
      "  [ 30]\n",
      "  [132]\n",
      "  [255]\n",
      "  [ 37]\n",
      "  [122]\n",
      "  [ 60]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 80]\n",
      "  [ 69]\n",
      "  [142]\n",
      "  [180]\n",
      "  [142]\n",
      "  [ 57]\n",
      "  [ 64]\n",
      "  [ 78]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 85]\n",
      "  [ 21]\n",
      "  [185]\n",
      "  [227]\n",
      "  [ 37]\n",
      "  [143]\n",
      "  [ 63]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 83]\n",
      "  [ 71]\n",
      "  [136]\n",
      "  [194]\n",
      "  [126]\n",
      "  [ 46]\n",
      "  [ 69]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 78]\n",
      "  [ 38]\n",
      "  [139]\n",
      "  [185]\n",
      "  [ 60]\n",
      "  [151]\n",
      "  [ 58]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  4]\n",
      "  [ 81]\n",
      "  [ 74]\n",
      "  [145]\n",
      "  [177]\n",
      "  [ 78]\n",
      "  [ 49]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 72]\n",
      "  [ 63]\n",
      "  [ 80]\n",
      "  [156]\n",
      "  [117]\n",
      "  [153]\n",
      "  [ 55]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 10]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [157]\n",
      "  [163]\n",
      "  [ 61]\n",
      "  [ 55]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 71]\n",
      "  [ 60]\n",
      "  [ 98]\n",
      "  [156]\n",
      "  [132]\n",
      "  [ 58]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 13]\n",
      "  [ 77]\n",
      "  [ 74]\n",
      "  [157]\n",
      "  [143]\n",
      "  [ 43]\n",
      "  [ 61]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 71]\n",
      "  [ 58]\n",
      "  [ 80]\n",
      "  [157]\n",
      "  [120]\n",
      "  [ 66]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [ 81]\n",
      "  [ 74]\n",
      "  [156]\n",
      "  [114]\n",
      "  [ 35]\n",
      "  [ 72]\n",
      "  [ 71]\n",
      "  [ 75]\n",
      "  [ 78]\n",
      "  [ 72]\n",
      "  [ 66]\n",
      "  [ 80]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 64]\n",
      "  [ 63]\n",
      "  [165]\n",
      "  [119]\n",
      "  [ 68]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 23]\n",
      "  [ 85]\n",
      "  [ 81]\n",
      "  [177]\n",
      "  [ 57]\n",
      "  [ 52]\n",
      "  [ 77]\n",
      "  [ 71]\n",
      "  [ 78]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 64]\n",
      "  [ 37]\n",
      "  [173]\n",
      "  [ 95]\n",
      "  [ 72]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 26]\n",
      "  [ 81]\n",
      "  [ 86]\n",
      "  [160]\n",
      "  [ 20]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 78]\n",
      "  [ 80]\n",
      "  [ 89]\n",
      "  [ 78]\n",
      "  [ 81]\n",
      "  [ 83]\n",
      "  [ 80]\n",
      "  [ 74]\n",
      "  [ 20]\n",
      "  [177]\n",
      "  [ 77]\n",
      "  [ 74]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 49]\n",
      "  [ 77]\n",
      "  [ 91]\n",
      "  [200]\n",
      "  [  0]\n",
      "  [ 83]\n",
      "  [ 95]\n",
      "  [ 86]\n",
      "  [ 88]\n",
      "  [ 88]\n",
      "  [ 89]\n",
      "  [ 88]\n",
      "  [ 89]\n",
      "  [ 88]\n",
      "  [ 83]\n",
      "  [ 89]\n",
      "  [ 86]\n",
      "  [  0]\n",
      "  [191]\n",
      "  [ 78]\n",
      "  [ 80]\n",
      "  [ 24]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 54]\n",
      "  [ 71]\n",
      "  [108]\n",
      "  [165]\n",
      "  [  0]\n",
      "  [ 24]\n",
      "  [ 57]\n",
      "  [ 52]\n",
      "  [ 57]\n",
      "  [ 60]\n",
      "  [ 60]\n",
      "  [ 60]\n",
      "  [ 63]\n",
      "  [ 63]\n",
      "  [ 77]\n",
      "  [ 89]\n",
      "  [ 52]\n",
      "  [  0]\n",
      "  [211]\n",
      "  [ 97]\n",
      "  [ 77]\n",
      "  [ 61]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 68]\n",
      "  [ 91]\n",
      "  [117]\n",
      "  [137]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [216]\n",
      "  [ 94]\n",
      "  [ 97]\n",
      "  [ 57]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 54]\n",
      "  [115]\n",
      "  [105]\n",
      "  [185]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  1]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [153]\n",
      "  [ 78]\n",
      "  [106]\n",
      "  [ 37]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [ 61]\n",
      "  [ 41]\n",
      "  [103]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [106]\n",
      "  [ 47]\n",
      "  [ 69]\n",
      "  [ 23]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]], shape=(28, 28, 1), dtype=uint8)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 09:01:27.567275: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for item in mnist_train.take(1):\n",
    "    print(type(item))\n",
    "    print(item.keys())\n",
    "    print(item['image'])\n",
    "    print(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeac7308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='fashion_mnist',\n",
      "    full_name='fashion_mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
      "    \"\"\",\n",
      "    homepage='https://github.com/zalandoresearch/fashion-mnist',\n",
      "    data_path='/home/studio-lab-user/tensorflow_datasets/fashion_mnist/3.0.1',\n",
      "    download_size=29.45 MiB,\n",
      "    dataset_size=36.42 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{DBLP:journals/corr/abs-1708-07747,\n",
      "      author    = {Han Xiao and\n",
      "                   Kashif Rasul and\n",
      "                   Roland Vollgraf},\n",
      "      title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n",
      "                   Algorithms},\n",
      "      journal   = {CoRR},\n",
      "      volume    = {abs/1708.07747},\n",
      "      year      = {2017},\n",
      "      url       = {http://arxiv.org/abs/1708.07747},\n",
      "      archivePrefix = {arXiv},\n",
      "      eprint    = {1708.07747},\n",
      "      timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n",
      "      biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n",
      "      bibsource = {dblp computer science bibliography, https://dblp.org}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mnist_test, info = tfds.load(name=\"fashion_mnist\", with_info=\"true\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9013edc",
   "metadata": {},
   "source": [
    "## 케라스 모델에서 TFDS 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202cc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = \\\n",
    "    mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c4b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels), (test_images, test_labels) = \\\n",
    "    tfds.as_numpy(tfds.load('fashion_mnist',\n",
    "                            split = ['train', 'test'], \n",
    "                            batch_size=-1, \n",
    "                            as_supervised=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d90467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5298 - accuracy: 0.8112\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4002 - accuracy: 0.8554\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3669 - accuracy: 0.8658\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3453 - accuracy: 0.8740\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3283 - accuracy: 0.8783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc96be2580>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training_images, training_labels), (test_images, test_labels) = \\\n",
    "    tfds.load('fashion_mnist', \n",
    "              split = ['train', 'test'],\n",
    "              batch_size=-1, \n",
    "              as_supervised=True)\n",
    "\n",
    "training_images = tf.cast(training_images, tf.float32) / 255.0\n",
    "test_images = tf.cast(test_images, tf.float32) / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8731883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 153.59 MiB (download: 153.59 MiB, generated: Unknown size, total: 153.59 MiB) to /home/studio-lab-user/tensorflow_datasets/horses_or_humans/3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c662d88de247159fed330005fa6bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97d56f714a947dbbc83e8c641397be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/1027 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/studio-lab-user/tensorflow_datasets/horses_or_humans/3.0.0.incompleteBVCEPZ/horses_or_humans-t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/studio-lab-user/tensorflow_datasets/horses_or_humans/3.0.0.incompleteBVCEPZ/horses_or_humans-t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset horses_or_humans downloaded and prepared to /home/studio-lab-user/tensorflow_datasets/horses_or_humans/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Epoch 1/10\n",
      "103/103 [==============================] - 48s 456ms/step - loss: 0.7667 - accuracy: 0.8695\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 45s 436ms/step - loss: 0.2182 - accuracy: 0.9270\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 45s 433ms/step - loss: 0.0574 - accuracy: 0.9825\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 45s 432ms/step - loss: 0.1969 - accuracy: 0.9659\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 44s 431ms/step - loss: 0.1637 - accuracy: 0.9552\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 44s 430ms/step - loss: 0.0281 - accuracy: 0.9883\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 44s 429ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 44s 428ms/step - loss: 7.8404e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 45s 434ms/step - loss: 4.0740e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 47s 456ms/step - loss: 3.1234e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc6b0109d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tfds.load('horses_or_humans', split='train', as_supervised=True) \n",
    "\n",
    "train_batches = data.shuffle(100).batch(10)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', \n",
    "                           input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_batches, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11dd9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = tfds.load('horses_or_humans', split='test', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df701a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batches = val_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f5abb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "103/103 [==============================] - 49s 474ms/step - loss: 2.5147e-05 - accuracy: 1.0000 - val_loss: 5.0156 - val_accuracy: 0.8281\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 47s 460ms/step - loss: 2.0809e-05 - accuracy: 1.0000 - val_loss: 5.0860 - val_accuracy: 0.8281\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 48s 462ms/step - loss: 1.7477e-05 - accuracy: 1.0000 - val_loss: 5.1535 - val_accuracy: 0.8281\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 48s 462ms/step - loss: 1.4521e-05 - accuracy: 1.0000 - val_loss: 5.2090 - val_accuracy: 0.8281\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 48s 465ms/step - loss: 1.1949e-05 - accuracy: 1.0000 - val_loss: 5.2506 - val_accuracy: 0.8281\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 47s 458ms/step - loss: 9.6102e-06 - accuracy: 1.0000 - val_loss: 5.2695 - val_accuracy: 0.8281\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 47s 460ms/step - loss: 7.5340e-06 - accuracy: 1.0000 - val_loss: 5.2794 - val_accuracy: 0.8320\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 47s 461ms/step - loss: 6.0835e-06 - accuracy: 1.0000 - val_loss: 5.3341 - val_accuracy: 0.8281\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 48s 465ms/step - loss: 5.0052e-06 - accuracy: 1.0000 - val_loss: 5.3863 - val_accuracy: 0.8281\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 48s 464ms/step - loss: 4.0223e-06 - accuracy: 1.0000 - val_loss: 5.3910 - val_accuracy: 0.8281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc6b3320d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_batches, epochs=10,\n",
    "          validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4241e",
   "metadata": {},
   "source": [
    "### 특정 버전의 데이터셋 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e825b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, info = tfds.load(\"horses_or_humans:3.0.0\", with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad1df2",
   "metadata": {},
   "source": [
    "## 데이터 증식을 위해 매핑 함수 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec01220",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfds.load('horses_or_humans', split='train', as_supervised=True)\n",
    "train_batches = data.shuffle(100).batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd15b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentimages(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/255)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7908101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.map(augmentimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9f922f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train.shuffle(100).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a8f4b",
   "metadata": {},
   "source": [
    "### 텐서플로 애드온 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a255512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.16.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.16.1 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc21f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "def augmentimages(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/255)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tfa.image.rotate(image, 40, interpolation='NEAREST')\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1af629",
   "metadata": {},
   "source": [
    "## 사용자 정의 분할 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b49b8-0e6b-4958-aca9-8b6658e547aa",
   "metadata": {},
   "source": [
    "다운로드 파일을 찾을 수 없다는 에러(404)가 나는 경우 다운로드 경로를 직접 지정하여 사용할 수 있습니다. 예를 들어 다음처럼 cats_vs_dogs 모듈의 `_URL` 변수에 다운로드 경로를 지정합니다. cats_vs_dogs 데이터셋의 최신 다운로드 경로는 https://www.microsoft.com/en-us/download/details.aspx?id=54765 를 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25f56d88-750b-4acc-a9b5-4a070497f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfds.image_classification.cats_vs_dogs._URL = \\\n",
    "#    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aff3fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 786.68 MiB (download: 786.68 MiB, generated: Unknown size, total: 786.68 MiB) to /home/studio-lab-user/tensorflow_datasets/cats_vs_dogs/4.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210d9440e9bc4eecb4480a5254958693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5454c1c96ff84ec6bbb2881e43166d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/23262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:1738 images were corrupted and were skipped\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/studio-lab-user/tensorflow_datasets/cats_vs_dogs/4.0.0.incompleteXHMC48/cats_vs_dogs-train.tfr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cats_vs_dogs downloaded and prepared to /home/studio-lab-user/tensorflow_datasets/cats_vs_dogs/4.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data = tfds.load('cats_vs_dogs', split='train', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84be5a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = tfds.load('cats_vs_dogs', split='train[:10000]', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b87b9123",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfds.load('cats_vs_dogs', split='train[:20%]', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28f5bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfds.load('cats_vs_dogs', split='train[-1000:]+train[:1000]', \n",
    "                 as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b11136c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tfds.load('cats_vs_dogs', split='train[:80%]', \n",
    "                       as_supervised=True)\n",
    "validation_data = tfds.load('cats_vs_dogs', split='train[80%:90%]', \n",
    "                            as_supervised=True)\n",
    "test_data = tfds.load('cats_vs_dogs', split='train[-10%:]',\n",
    "                       as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bea8477c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n",
      "Warning: unknown JFIF revision number 0.00\n",
      "Corrupt JPEG data: 396 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 128 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 239 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 228 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18610\n"
     ]
    }
   ],
   "source": [
    "train_length = [i for i,_ in enumerate(train_data)][-1] + 1\n",
    "print(train_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d59998ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18610"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.experimental.cardinality(train_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d91db041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "train_data, info = tfds.load('fashion_mnist', with_info=True)\n",
    "print(info.splits['train'].num_examples, info.splits['test'].num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f869e",
   "metadata": {},
   "source": [
    "## TFRecord 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "766f4588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /home/studio-lab-user/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165b4d52ef8f454cb112a585c3f16bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /home/studio-lab-user/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    full_name='mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    The MNIST database of handwritten digits.\n",
      "    \"\"\",\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    data_path='/home/studio-lab-user/tensorflow_datasets/mnist/3.0.1',\n",
      "    download_size=11.06 MiB,\n",
      "    dataset_size=21.00 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data, info = tfds.load(\"mnist\", with_info=True)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6024b18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(), dtype=string, numpy=b\"\\n\\x85\\x03\\n\\xf2\\x02\\n\\x05image\\x12\\xe8\\x02\\n\\xe5\\x02\\n\\xe2\\x02\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x00\\x00\\x01)IDAT(\\x91\\xc5\\xd2\\xbdK\\xc3P\\x14\\x05\\xf0S(v\\x13)\\x04,.\\x82\\xc5Aq\\xac\\xedb\\x1d\\xdc\\n.\\x12\\x87n\\x0e\\x82\\x93\\x7f@Q\\xb2\\x08\\xba\\tbQ0.\\xe2\\xe2\\xd4\\xb1\\xa2h\\x9c\\x82\\xba\\x8a(\\nq\\xf0\\x83Fh\\x95\\n6\\x88\\xe7R\\x87\\x88\\xf9\\xa8Y\\xf5\\x0e\\x8f\\xc7\\xfd\\xdd\\x0b\\x87\\xc7\\x03\\xfe\\xbeb\\x9d\\xadT\\x927Q\\xe3\\xe9\\x07:\\xab\\xbf\\xf4\\xf3\\xcf\\xf6\\x8a\\xd9\\x14\\xd29\\xea\\xb0\\x1eKH\\xde\\xab\\xea%\\xaba\\x1b=\\xa4P/\\xf5\\x02\\xd7\\\\\\x07\\x00\\xc4=,L\\xc0,>\\x01@2\\xf6\\x12\\xde\\x9c\\xde[t/\\xb3\\x0e\\x87\\xa2\\xe2\\xc2\\xe0A<\\xca\\xb26\\xd5(\\x1b\\xa9\\xd3\\xe8\\x0e\\xf5\\x86\\x17\\xceE\\xdarV\\xae\\xb7_\\xf3AR\\r!I\\xf7(\\x06m\\xaaE\\xbb\\xb6\\xac\\r*\\x9b$e<\\xb8\\xd7\\xa2\\x0e\\x00\\xd0l\\x92\\xb2\\xd5\\x15\\xcc\\xae'\\x00\\xf4m\\x08O'+\\xc2y\\x9f\\x8d\\xc9\\x15\\x80\\xfe\\x99[q\\x962@CN|i\\xf7\\xa9!=\\xd7 \\xab\\x19\\x00\\xc8\\xd6\\xb8\\xeb\\xa1\\xf0\\xd8l\\xca\\xfb]\\xee\\xfb]*\\x9fV\\xe1\\x07\\xb7\\xc9\\x8b55\\xe7M\\xef\\xb0\\x04\\xc0\\xfd&\\x89\\x01<\\xbe\\xf9\\x03*\\x8a\\xf5\\x81\\x7f\\xaa/2y\\x87ks\\xec\\x1e\\xc1\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\x02\">\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "filename = os.path.join(os.path.expanduser('~') + \n",
    "                        '/tensorflow_datasets/mnist/3.0.1/mnist-test.tfrecord-00000-of-00001')\n",
    "raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e136603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <tf.Tensor: shape=(), dtype=string, numpy=b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x00\\x00\\x01)IDAT(\\x91\\xc5\\xd2\\xbdK\\xc3P\\x14\\x05\\xf0S(v\\x13)\\x04,.\\x82\\xc5Aq\\xac\\xedb\\x1d\\xdc\\n.\\x12\\x87n\\x0e\\x82\\x93\\x7f@Q\\xb2\\x08\\xba\\tbQ0.\\xe2\\xe2\\xd4\\xb1\\xa2h\\x9c\\x82\\xba\\x8a(\\nq\\xf0\\x83Fh\\x95\\n6\\x88\\xe7R\\x87\\x88\\xf9\\xa8Y\\xf5\\x0e\\x8f\\xc7\\xfd\\xdd\\x0b\\x87\\xc7\\x03\\xfe\\xbeb\\x9d\\xadT\\x927Q\\xe3\\xe9\\x07:\\xab\\xbf\\xf4\\xf3\\xcf\\xf6\\x8a\\xd9\\x14\\xd29\\xea\\xb0\\x1eKH\\xde\\xab\\xea%\\xaba\\x1b=\\xa4P/\\xf5\\x02\\xd7\\\\\\x07\\x00\\xc4=,L\\xc0,>\\x01@2\\xf6\\x12\\xde\\x9c\\xde[t/\\xb3\\x0e\\x87\\xa2\\xe2\\xc2\\xe0A<\\xca\\xb26\\xd5(\\x1b\\xa9\\xd3\\xe8\\x0e\\xf5\\x86\\x17\\xceE\\xdarV\\xae\\xb7_\\xf3AR\\r!I\\xf7(\\x06m\\xaaE\\xbb\\xb6\\xac\\r*\\x9b$e<\\xb8\\xd7\\xa2\\x0e\\x00\\xd0l\\x92\\xb2\\xd5\\x15\\xcc\\xae'\\x00\\xf4m\\x08O'+\\xc2y\\x9f\\x8d\\xc9\\x15\\x80\\xfe\\x99[q\\x962@CN|i\\xf7\\xa9!=\\xd7 \\xab\\x19\\x00\\xc8\\xd6\\xb8\\xeb\\xa1\\xf0\\xd8l\\xca\\xfb]\\xee\\xfb]*\\x9fV\\xe1\\x07\\xb7\\xc9\\x8b55\\xe7M\\xef\\xb0\\x04\\xc0\\xfd&\\x89\\x01<\\xbe\\xf9\\x03*\\x8a\\xf5\\x81\\x7f\\xaa/2y\\x87ks\\xec\\x1e\\xc1\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82\">, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=2>}\n"
     ]
    }
   ],
   "source": [
    "# 특성 디스크립션을 만듭니다.\n",
    "feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    # 위에서 만든 딕셔너리로 입력을 파싱합니다.\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "for parsed_record in parsed_dataset.take(1):\n",
    "    print((parsed_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc32d9c",
   "metadata": {},
   "source": [
    "## 텐서플로에서 데이터 관리를 위한 ETL 프로세스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5196472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33/33 [==============================] - 58s 2s/step - loss: 0.4347 - accuracy: 0.8179 - val_loss: 92.5671 - val_accuracy: 0.7148\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.1094 - accuracy: 0.9572 - val_loss: 223.7977 - val_accuracy: 0.6602\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.0437 - accuracy: 0.9825 - val_loss: 525.5711 - val_accuracy: 0.6523\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.0318 - accuracy: 0.9903 - val_loss: 293.2694 - val_accuracy: 0.7188\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 55s 2s/step - loss: 0.0776 - accuracy: 0.9776 - val_loss: 356.0112 - val_accuracy: 0.7109\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.0284 - accuracy: 0.9912 - val_loss: 412.7202 - val_accuracy: 0.6484\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 55s 2s/step - loss: 0.0453 - accuracy: 0.9834 - val_loss: 357.0209 - val_accuracy: 0.6016\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 564.5863 - val_accuracy: 0.6797\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.0066 - accuracy: 0.9971 - val_loss: 660.9488 - val_accuracy: 0.6914\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 55s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 821.2891 - val_accuracy: 0.6836\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# 모델 정의 시작 #\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', \n",
    "                           input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "# 모델 정의 끝 #\n",
    "\n",
    "# 추출 단계 시작 #\n",
    "data = tfds.load('horses_or_humans', split='train', \n",
    "                 as_supervised=True)\n",
    "val_data = tfds.load('horses_or_humans', split='test', \n",
    "                     as_supervised=True)\n",
    "# 추출 단계 끝 #\n",
    "\n",
    "# 변환 단계 시작 #\n",
    "def augmentimages(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/255)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tfa.image.rotate(image, 40, interpolation='NEAREST')\n",
    "    return image, label\n",
    "\n",
    "train = data.map(augmentimages)\n",
    "train_batches = train.shuffle(100).batch(32)\n",
    "validation_batches = val_data.batch(32)\n",
    "# 변환 단계 끝 #\n",
    "\n",
    "# 로드 단계 시작 #\n",
    "history = model.fit(train_batches, epochs=10, \n",
    "                    validation_data=validation_batches)\n",
    "# 로드 단계 끝 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e35d16",
   "metadata": {},
   "source": [
    "### 훈련 속도 향상을 위한 ETL 병렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45b0c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tfds.load('cats_vs_dogs', split='train', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7b36946",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = os.path.join(\n",
    "    os.path.expanduser('~') + \n",
    "    '/tensorflow_datasets/cats_vs_dogs/4.0.0/cats_vs_dogs-train.tfrecord*'\n",
    ")\n",
    "files = tf.data.Dataset.list_files(file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97f5cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = files.interleave(\n",
    "    tf.data.TFRecordDataset, \n",
    "    cycle_length=4,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7ba844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(serialized_example):\n",
    "    feature_description={\n",
    "        \"image\": tf.io.FixedLenFeature((), tf.string, \"\"),\n",
    "        \"label\": tf.io.FixedLenFeature((), tf.int64, -1),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example, feature_description\n",
    "    )\n",
    "    image = tf.io.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255\n",
    "    image = tf.image.resize(image, (300,300))\n",
    "    return image, example['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54cc5785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(cores)\n",
    "train_dataset = train_dataset.map(read_tfrecord, num_parallel_calls=cores)\n",
    "train_dataset = train_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69f317e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(1024).batch(32)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640def46",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2977/3460828210.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=10, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
